# -*- coding: utf-8 -*-
"""semester_project_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ciNJ15GNx_7Ih7CJS2Ii_S6n3-_isyG3
"""

import os
import requests
import pandas as pd
from datetime import datetime

API_KEY = "86b67e98f5134a7386ce62902a756492"
BLS_URL = "https://api.bls.gov/publicAPI/v2/timeseries/data/"

SERIES_IDS = [
    "CES0000000001",
    "LNS14000000",
    "CES0500000003",
    "LNS11300000",
    "LNS12300000"
]

LOCAL_DATA_FILE = "bls_data.csv"

def fetch_bls_data(start_year, end_year):
    payload = {
        "seriesid": SERIES_IDS,
        "startyear": start_year,
        "endyear": end_year,
        "registrationkey": API_KEY
    }
    response = requests.post(BLS_URL, json=payload)
    if response.status_code == 200:
        data = response.json()
        if data.get("status") == "REQUEST_SUCCEEDED":
            return data
        else:
            print("Error from API:", data.get("message"))
    else:
        print(f"HTTP Error: {response.status_code}")
    return None

def process_bls_data(data):
    records = []
    for series in data["Results"]["series"]:
        series_id = series["seriesID"]
        for entry in series["data"]:
            records.append({
                "series_id": series_id,
                "date": datetime.strptime(f"{entry['year']} {entry['periodName']}", "%Y %B"),
                "value": float(entry["value"])
            })
    return pd.DataFrame(records)

def load_local_data(file_path):
    if os.path.exists(file_path):
        return pd.read_csv(file_path, parse_dates=["date"])
    return pd.DataFrame(columns=["series_id", "date", "value"])

def save_local_data(data, file_path):
    data.to_csv(file_path, index=False)

def update_data():
    local_data = load_local_data(LOCAL_DATA_FILE)

    if not local_data.empty:
        latest_date = local_data["date"].max()
        start_year = latest_date.year + 1
    else:
        start_year = datetime.now().year - 1

    end_year = datetime.now().year
    new_data = fetch_bls_data(start_year, end_year)

    if new_data:
        new_data_df = process_bls_data(new_data)
        combined_data = pd.concat([local_data, new_data_df]).drop_duplicates()
        save_local_data(combined_data, LOCAL_DATA_FILE)
        return combined_data
    else:
        print("No new data fetched.")
        return local_data

pip install streamlit

import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt

# Load your processed data
data = pd.read_csv("bls_data.csv")  # Replace with the path to your data file

# Streamlit app layout
st.title('BLS Data Dashboard')

# Create a plot
fig, ax = plt.subplots(figsize=(12, 6))
for series_id in data["series_id"].unique():
    series_data = data[data["series_id"] == series_id]
    ax.plot(series_data["date"], series_data["value"], label=series_id)

ax.set_title('BLS Data')
ax.set_xlabel('Date')
ax.set_ylabel('Value')
ax.legend()
ax.grid(True)

# Display the plot in Streamlit
st.pyplot(fig)

# Optionally display the raw data
st.subheader("Raw Data")
st.write(data)